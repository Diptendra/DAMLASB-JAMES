{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cloudera\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `DAMLASB': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir DAMLASB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cloudera/DAMLASB\n"
     ]
    }
   ],
   "source": [
    "cd DAMLASB/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cloudera/DAMLASB\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\r\n",
      "drwxrwxr-x 2 cloudera cloudera 4096 Aug 30 02:39 map_reduce\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `map_reduce': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir map_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cloudera/DAMLASB/map_reduce\n"
     ]
    }
   ],
   "source": [
    "cd map_reduce/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrjob\n",
      "  Downloading mrjob-0.5.4-py2.py3-none-any.whl (284kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 707kB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): boto>=2.35.0 in /home/cloudera/anaconda2/lib/python2.7/site-packages (from mrjob)\n",
      "Requirement already satisfied (use --upgrade to upgrade): PyYAML>=3.08 in /home/cloudera/anaconda2/lib/python2.7/site-packages (from mrjob)\n",
      "Collecting filechunkio (from mrjob)\n",
      "  Downloading filechunkio-1.8.tar.gz\n",
      "Collecting google-api-python-client>=1.5.0 (from mrjob)\n",
      "  Downloading google_api_python_client-1.5.3-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 4.0MB/s \n",
      "\u001b[?25hCollecting httplib2<1,>=0.8 (from google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading httplib2-0.9.2.zip (210kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 307kB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): six<2,>=1.6.1 in /home/cloudera/anaconda2/lib/python2.7/site-packages (from google-api-python-client>=1.5.0->mrjob)\n",
      "Collecting uritemplate<1,>=0.6 (from google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading uritemplate-0.6.tar.gz\n",
      "Collecting oauth2client<4.0.0,>=1.5.0 (from google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading oauth2client-3.0.0.tar.gz (77kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 75kB/s \n",
      "\u001b[?25hCollecting simplejson>=2.5.0 (from uritemplate<1,>=0.6->google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading simplejson-3.8.2.tar.gz (76kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 99kB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): pyasn1>=0.1.7 in /home/cloudera/anaconda2/lib/python2.7/site-packages (from oauth2client<4.0.0,>=1.5.0->google-api-python-client>=1.5.0->mrjob)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client<4.0.0,>=1.5.0->google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading pyasn1_modules-0.0.8-py2.py3-none-any.whl\n",
      "Collecting rsa>=3.1.4 (from oauth2client<4.0.0,>=1.5.0->google-api-python-client>=1.5.0->mrjob)\n",
      "  Downloading rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.9MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: filechunkio, httplib2, uritemplate, oauth2client, simplejson\n",
      "  Running setup.py bdist_wheel for filechunkio ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/cloudera/.cache/pip/wheels/26/ae/6a/d93b918a6a07a0afab4b1c51681047657d1300160ab7045565\n",
      "  Running setup.py bdist_wheel for httplib2 ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/cloudera/.cache/pip/wheels/c7/67/60/e0be8ccfc1e08f8ff1f50d99ea5378e204580ea77b0169fb55\n",
      "  Running setup.py bdist_wheel for uritemplate ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/cloudera/.cache/pip/wheels/2d/dc/57/124fcb62028d04cf74f6c7f5261f5deb29b3f6022eec179064\n",
      "  Running setup.py bdist_wheel for oauth2client ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/cloudera/.cache/pip/wheels/b3/44/e9/e56c5a2ca8869305f38254df012fb16b2807eb9d5d55291e8b\n",
      "  Running setup.py bdist_wheel for simplejson ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/cloudera/.cache/pip/wheels/e4/32/71/60b361b0d05433eb9d1dd3d47619931c08cc4e387dc494ad3c\n",
      "Successfully built filechunkio httplib2 uritemplate oauth2client simplejson\n",
      "Installing collected packages: filechunkio, httplib2, simplejson, uritemplate, pyasn1-modules, rsa, oauth2client, google-api-python-client, mrjob\n",
      "Successfully installed filechunkio-1.8 google-api-python-client-1.5.3 httplib2-0.9.2 mrjob-0.5.4 oauth2client-3.0.0 pyasn1-modules-0.0.8 rsa-3.4.2 simplejson-3.8.2 uritemplate-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!export PATH=\"/home/cloudera/anaconda/bin:$PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! find / -name \"hadoop*streaming*\" -print 1> tmp.hadoopJar 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/share/doc/hadoop-2.6.0+cdh5.4.2+567/hadoop-streaming\r\n",
      "/usr/lib/oozie/oozie-sharelib-mr1/lib/mapreduce-streaming/hadoop-streaming.jar\r\n",
      "/usr/lib/oozie/oozie-sharelib-yarn/lib/mapreduce-streaming/hadoop-streaming.jar\r\n",
      "/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.4.2.jar\r\n",
      "/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\r\n",
      "/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar\r\n",
      "/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar\r\n",
      "/usr/jars/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar\r\n",
      "/usr/jars/hadoop-streaming-2.6.0-cdh5.4.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "!cat tmp.hadoopJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrwxrwxrwx 1 root root 56 Jun  9  2015 /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar -> ../../../../jars/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "-rw-rw-r-- 1 cloudera cloudera  585 Aug 30 07:29 tmp.hadoopJar\r\n",
      "drwxrwxr-x 2 cloudera cloudera 4096 Aug 30 07:32 WordCount\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount/word_count_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/word_count_mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line= re.findall(r'[a-z]+', line.lower())\n",
    "    line=\" \".join(line)\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "-rw-rw-r-- 1 cloudera cloudera  585 Aug 30 07:29 tmp.hadoopJar\r\n",
      "drwxrwxr-x 2 cloudera cloudera 4096 Aug 30 07:48 WordCount\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_count_mapper.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls WordCount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "\r\n",
      "import sys\r\n",
      "import re\r\n",
      "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\r\n",
      "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\r\n",
      "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\r\n",
      "\r\n",
      "for line in sys.stdin:\r\n",
      "    line= re.findall(r'[a-z]+', line.lower())\r\n",
      "    line=\" \".join(line)\r\n",
      "    words = line.split()\r\n",
      "    for word in words:\r\n",
      "        print '%s\\t%s' % (word, 1)"
     ]
    }
   ],
   "source": [
    "!cat WordCount/word_count_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount/word_count_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/word_count_reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x WordCount/word_count_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x WordCount/word_count_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are  you\r\n",
      "foo\t1\r\n",
      "foo\t1\r\n",
      "quux\t1\r\n",
      "labs\t1\r\n",
      "foo\t1\r\n",
      "bar\t1\r\n",
      "quux\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/word_count_mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are  you\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/word_count_mapper.py | sort -k1,1 | WordCount/word_count_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-pwd: Unknown command\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  173k  100  173k    0     0  76590      0  0:00:02  0:00:02 --:--:-- 87446\n"
     ]
    }
   ],
   "source": [
    "!curl 'http://www.gutenberg.org/cache/epub/28885/pg28885.txt' -o alice_input_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 184\r\n",
      "-rw-rw-r-- 1 cloudera cloudera    585 Aug 30 07:29 tmp.hadoopJar\r\n",
      "drwxrwxr-x 2 cloudera cloudera   4096 Aug 30 08:03 WordCount\r\n",
      "-rw-rw-r-- 1 cloudera cloudera 177428 Aug 30 08:18 alice_input_file.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal alice_input_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 cloudera cloudera     177428 2016-08-30 08:21 alice_input_file.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/08/30 08:30:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted wordcount-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcount-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cloudera/DAMLASB/map_reduce\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/08/30 21:55:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted wordcount-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.4.2.jar] /tmp/streamjob3707439227408718973.jar tmpDir=null\n",
      "16/08/30 21:55:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/30 21:55:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/30 21:55:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/08/30 21:55:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/08/30 21:55:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1472567065777_0008\n",
      "16/08/30 21:55:32 INFO impl.YarnClientImpl: Submitted application application_1472567065777_0008\n",
      "16/08/30 21:55:32 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1472567065777_0008/\n",
      "16/08/30 21:55:32 INFO mapreduce.Job: Running job: job_1472567065777_0008\n",
      "16/08/30 21:55:42 INFO mapreduce.Job: Job job_1472567065777_0008 running in uber mode : false\n",
      "16/08/30 21:55:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/08/30 21:55:55 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/08/30 21:55:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/08/30 21:56:13 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "16/08/30 21:56:15 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "16/08/30 21:56:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/08/30 21:56:16 INFO mapreduce.Job: Job job_1472567065777_0008 completed successfully\n",
      "16/08/30 21:56:16 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=47435\n",
      "\t\tFILE: Number of bytes written=659474\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=179062\n",
      "\t\tHDFS: Number of bytes written=29633\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24061\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=49216\n",
      "\t\tTotal time spent by all map tasks (ms)=24061\n",
      "\t\tTotal time spent by all reduce tasks (ms)=49216\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24061\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=49216\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=24638464\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=50397184\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4046\n",
      "\t\tMap output records=31385\n",
      "\t\tMap output bytes=221457\n",
      "\t\tMap output materialized bytes=47453\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=31385\n",
      "\t\tCombine output records=4224\n",
      "\t\tReduce input groups=3134\n",
      "\t\tReduce shuffle bytes=47453\n",
      "\t\tReduce input records=4224\n",
      "\t\tReduce output records=3134\n",
      "\t\tSpilled Records=8448\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=830\n",
      "\t\tCPU time spent (ms)=4870\n",
      "\t\tPhysical memory (bytes) snapshot=985903104\n",
      "\t\tVirtual memory (bytes) snapshot=7526326272\n",
      "\t\tTotal committed heap usage (bytes)=982327296\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=9\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=178826\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29633\n",
      "16/08/30 21:56:16 INFO streaming.StreamJob: Output directory: wordcount-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcount-output\n",
    "!hadoop jar /usr/jars/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar \\\n",
    "   -mapper /home/cloudera/DAMLASB/map_reduce/WordCount/word_count_mapper.py \\\n",
    "   -reducer /home/cloudera/DAMLASB/map_reduce/WordCount/word_count_reducer.py \\\n",
    "   -combiner /home/cloudera/DAMLASB/map_reduce/WordCount/word_count_reducer.py \\\n",
    "   -input alice_input_file.txt \\\n",
    "   -output wordcount-output  \\\n",
    "   -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.1: Find longest word in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir longest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing longest_word/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest_word/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line= re.findall(r'[a-z]+', line.lower())\n",
    "    line=\" \".join(line)\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        val=len(word)\n",
    "        print '%s\\t%s' % (word, val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest_word/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest_word/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "cur_key = None\n",
    "cur_length = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_length = int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            if cur_length > int(value):\n",
    "                cur_key=cur_key\n",
    "                cur_length=cur_length\n",
    "            else:\n",
    "                cur_key = key\n",
    "                cur_length = int(value)\n",
    "        else:\n",
    "            cur_key = key\n",
    "            cur_length = int(value)\n",
    "            \n",
    "print '%s\\t%s' % (cur_key, cur_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x longest_word/mapper.py\n",
    "!chmod a+x longest_word/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are  you\r\n",
      "lnsdfflkjbelfbd\t15\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux aaaaaaaa lnsdfflkjbelfbd\" | longest_word/mapper.py | sort -k1,1 | longest_word/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\n",
      "reporter:status:processing my message...how are  you\n",
      "reporter:counter:Reducer Counters,Calls,1\n",
      "unenforceability\t16\n"
     ]
    }
   ],
   "source": [
    "!cat alice_input_file.txt | longest_word/mapper.py | sort -k1,1 | longest_word/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `longest_word-output': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.4.2.jar] /tmp/streamjob5772638991259276456.jar tmpDir=null\n",
      "16/08/30 22:12:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/30 22:12:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/30 22:12:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/08/30 22:12:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/08/30 22:12:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1472567065777_0009\n",
      "16/08/30 22:12:04 INFO impl.YarnClientImpl: Submitted application application_1472567065777_0009\n",
      "16/08/30 22:12:04 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1472567065777_0009/\n",
      "16/08/30 22:12:04 INFO mapreduce.Job: Running job: job_1472567065777_0009\n",
      "16/08/30 22:12:13 INFO mapreduce.Job: Job job_1472567065777_0009 running in uber mode : false\n",
      "16/08/30 22:12:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/08/30 22:12:27 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/08/30 22:12:28 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/08/30 22:12:44 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "16/08/30 22:12:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/08/30 22:12:48 INFO mapreduce.Job: Job job_1472567065777_0009 completed successfully\n",
      "16/08/30 22:12:48 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=139\n",
      "\t\tFILE: Number of bytes written=564777\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=179062\n",
      "\t\tHDFS: Number of bytes written=57\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24419\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=45906\n",
      "\t\tTotal time spent by all map tasks (ms)=24419\n",
      "\t\tTotal time spent by all reduce tasks (ms)=45906\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24419\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=45906\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=25005056\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=47007744\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4046\n",
      "\t\tMap output records=31385\n",
      "\t\tMap output bytes=222114\n",
      "\t\tMap output materialized bytes=157\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=31385\n",
      "\t\tCombine output records=6\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=157\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=730\n",
      "\t\tCPU time spent (ms)=4460\n",
      "\t\tPhysical memory (bytes) snapshot=981622784\n",
      "\t\tVirtual memory (bytes) snapshot=7528628224\n",
      "\t\tTotal committed heap usage (bytes)=982327296\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=9\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=178826\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=57\n",
      "16/08/30 22:12:48 INFO streaming.StreamJob: Output directory: longest_word-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r longest_word-output\n",
    "!hdfs dfs -mkdir longest_word\n",
    "!hadoop jar /usr/jars/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar \\\n",
    "   -mapper /home/cloudera/DAMLASB/map_reduce/longest_word/mapper.py \\\n",
    "   -reducer /home/cloudera/DAMLASB/map_reduce/longest_word/reducer.py \\\n",
    "   -combiner /home/cloudera/DAMLASB/map_reduce/longest_word/reducer.py \\\n",
    "   -input alice_input_file.txt \\\n",
    "   -output longest_word-output  \\\n",
    "   -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unenforceability\t16\r\n",
      "representations\t15\r\n",
      "redistributing\t14\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat longest_word-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.2: Number of uppercase words and lower case words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir upper_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting upper_lower/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile upper_lower/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line= re.findall(r'[a-zA-Z]+', line)\n",
    "    line=\" \".join(line)\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if word[0].isupper()==True:\n",
    "            case='uppercase'\n",
    "            print '%s\\t%s\\t%s' % (word,1,case)\n",
    "        else:\n",
    "            case='lowercase'\n",
    "            print '%s\\t%s\\t%s' % (word,1,case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting upper_lower/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile upper_lower/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "upper_key = None\n",
    "lower_key = None\n",
    "upper_count = 0\n",
    "lower_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value, case = line.split()\n",
    "    if case =='uppercase':\n",
    "        if key == upper_key:\n",
    "            upper_count = upper_count\n",
    "        else:\n",
    "            if upper_key:\n",
    "                upper_key = key\n",
    "                upper_count = upper_count+1\n",
    "            else:\n",
    "                upper_key = key\n",
    "                upper_count = int(value)\n",
    "    else:\n",
    "        if key == lower_key:\n",
    "            lower_count = lower_count\n",
    "        else:\n",
    "            if lower_key:\n",
    "                lower_key = key\n",
    "                lower_count = lower_count+1\n",
    "            else:\n",
    "                lower_key = key\n",
    "                lower_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % ('lowercase_words', lower_count)\n",
    "print '%s\\t%s' % ('uppercase_words', upper_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x upper_lower/mapper.py\n",
    "!chmod a+x upper_lower/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\n",
      "reporter:status:processing my message...how are  you\n",
      "reporter:counter:Reducer Counters,Calls,1\n",
      "lowercase_words\t2882\n",
      "uppercase_words\t702\n"
     ]
    }
   ],
   "source": [
    "!cat alice_input_file.txt  | upper_lower/mapper.py | sort -k1,1 | upper_lower/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/08/31 03:19:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted upper_lower-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.4.2.jar] /tmp/streamjob315440929305937234.jar tmpDir=null\n",
      "16/08/31 03:20:00 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/31 03:20:00 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/31 03:20:01 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/08/31 03:20:01 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/08/31 03:20:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1472567065777_0029\n",
      "16/08/31 03:20:02 INFO impl.YarnClientImpl: Submitted application application_1472567065777_0029\n",
      "16/08/31 03:20:02 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1472567065777_0029/\n",
      "16/08/31 03:20:02 INFO mapreduce.Job: Running job: job_1472567065777_0029\n",
      "16/08/31 03:20:11 INFO mapreduce.Job: Job job_1472567065777_0029 running in uber mode : false\n",
      "16/08/31 03:20:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/08/31 03:20:24 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/08/31 03:20:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/08/31 03:20:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/08/31 03:20:34 INFO mapreduce.Job: Job job_1472567065777_0029 completed successfully\n",
      "16/08/31 03:20:34 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=598083\n",
      "\t\tFILE: Number of bytes written=1533715\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=179062\n",
      "\t\tHDFS: Number of bytes written=41\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23515\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6792\n",
      "\t\tTotal time spent by all map tasks (ms)=23515\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6792\n",
      "\t\tTotal vcore-seconds taken by all map tasks=23515\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6792\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=24079360\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6955008\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4046\n",
      "\t\tMap output records=31385\n",
      "\t\tMap output bytes=535307\n",
      "\t\tMap output materialized bytes=598089\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3584\n",
      "\t\tReduce shuffle bytes=598089\n",
      "\t\tReduce input records=31385\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=62770\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=377\n",
      "\t\tCPU time spent (ms)=3070\n",
      "\t\tPhysical memory (bytes) snapshot=684789760\n",
      "\t\tVirtual memory (bytes) snapshot=4512169984\n",
      "\t\tTotal committed heap usage (bytes)=673390592\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=178826\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=41\n",
      "16/08/31 03:20:34 INFO streaming.StreamJob: Output directory: upper_lower-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r upper_lower-output\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar \\\n",
    "   -mapper /home/cloudera/DAMLASB/map_reduce/upper_lower/mapper.py \\\n",
    "   -reducer /home/cloudera/DAMLASB/map_reduce/upper_lower/reducer.py \\\n",
    "   -input alice_input_file.txt \\\n",
    "   -output upper_lower-output  \\\n",
    "   -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase_words\t2882\r\n",
      "uppercase_words\t702\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat upper_lower-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW-2.3: Lowest temperature at Heathrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 42517  100 42517    0     0  50470      0 --:--:-- --:--:-- --:--:-- 76332\n"
     ]
    }
   ],
   "source": [
    "!curl 'http://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/heathrowdata.txt' -o uk_heathrow_input_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 236\r\n",
      "-rw-rw-r-- 1 cloudera cloudera    585 Aug 30 07:29 tmp.hadoopJar\r\n",
      "drwxrwxr-x 2 cloudera cloudera   4096 Aug 30 08:03 WordCount\r\n",
      "-rw-rw-r-- 1 cloudera cloudera 177428 Aug 30 08:18 alice_input_file.txt\r\n",
      "drwxrwxr-x 2 cloudera cloudera   4096 Aug 30 09:51 longest_word\r\n",
      "drwxrwxr-x 2 cloudera cloudera   4096 Aug 30 11:53 upper_lower\r\n",
      "-rw-rw-r-- 1 cloudera cloudera  42517 Aug 30 12:02 uk_heathrow_input_file.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heathrow (London Airport)\r\n",
      "Location 507800E 176700N, Lat 51.479 Lon -0.449, 25m amsl\r\n",
      "Estimated data is marked with a * after the value.\r\n",
      "Missing data (more than 2 days missing in month) is marked by  ---.\r\n",
      "Sunshine data taken from an automatic Kipp & Zonen sensor marked with a #, otherwise sunshine data taken from a Campbell Stokes recorder.\r\n",
      "   yyyy  mm   tmax    tmin      af    rain     sun\r\n",
      "              degC    degC    days      mm   hours\r\n",
      "   1948   1    8.9     3.3    ---     85.0    ---\r\n",
      "   1948   2    7.9     2.2    ---     26.0    ---\r\n",
      "   1948   3   14.2     3.8    ---     14.0    ---\r\n"
     ]
    }
   ],
   "source": [
    "!head uk_heathrow_input_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir lowest_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lowest_temp/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lowest_temp/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line= re.findall(r'[0-9.]+', line)\n",
    "    if line != []:\n",
    "        #words = line.split()\n",
    "        #line = line.strip()\n",
    "        if (len(line[0]) == 4 and 1 <= int(line[1]) <= 12):\n",
    "            print '%s\\t%s' % (line[0], line[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lowest_temp/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lowest_temp/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "min_temp = 0.0\n",
    "final_key = None\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == final_key:\n",
    "        if float(value) > min_temp:\n",
    "            min_temp = min_temp\n",
    "        else:\n",
    "            min_temp = value\n",
    "    else:\n",
    "        if final_key:\n",
    "            if float(value) > min_temp:\n",
    "                min_temp = min_temp\n",
    "            else:\n",
    "                min_temp = float(value)\n",
    "                final_key = key\n",
    "        else:\n",
    "            final_key = key\n",
    "            min_temp = float(value)\n",
    "\n",
    "print '%s\\t%s' % (final_key, min_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x lowest_temp/mapper.py\n",
    "!chmod a+x lowest_temp/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\n",
      "reporter:status:processing my message...how are  you\n",
      "reporter:counter:Reducer Counters,Calls,1\n",
      "1963\t0.0\n"
     ]
    }
   ],
   "source": [
    "!cat uk_heathrow_input_file.txt | lowest_temp/mapper.py | sort -k1,1 | lowest_temp/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 cloudera cloudera     177428 2016-08-30 08:21 alice_input_file.txt\n",
      "drwxr-xr-x   - cloudera cloudera          0 2016-08-30 09:49 wordcount-output\n",
      "Found 3 items\n",
      "-rw-r--r--   1 cloudera cloudera     177428 2016-08-30 08:21 alice_input_file.txt\n",
      "-rw-r--r--   1 cloudera cloudera      42517 2016-08-30 21:36 uk_heathrow_input_file.txt\n",
      "drwxr-xr-x   - cloudera cloudera          0 2016-08-30 09:49 wordcount-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls \n",
    "!hdfs dfs -copyFromLocal uk_heathrow_input_file.txt\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/08/31 02:41:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted lowest_temp-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.4.2.jar] /tmp/streamjob8687711466530980004.jar tmpDir=null\n",
      "16/08/31 02:41:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/31 02:41:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/08/31 02:41:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/08/31 02:41:59 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/08/31 02:41:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1472567065777_0021\n",
      "16/08/31 02:42:00 INFO impl.YarnClientImpl: Submitted application application_1472567065777_0021\n",
      "16/08/31 02:42:00 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1472567065777_0021/\n",
      "16/08/31 02:42:00 INFO mapreduce.Job: Running job: job_1472567065777_0021\n",
      "16/08/31 02:42:09 INFO mapreduce.Job: Job job_1472567065777_0021 running in uber mode : false\n",
      "16/08/31 02:42:09 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/08/31 02:42:22 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/08/31 02:42:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/08/31 02:42:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/08/31 02:42:30 INFO mapreduce.Job: Job job_1472567065777_0021 completed successfully\n",
      "16/08/31 02:42:30 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=28\n",
      "\t\tFILE: Number of bytes written=338743\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=46083\n",
      "\t\tHDFS: Number of bytes written=9\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21529\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5822\n",
      "\t\tTotal time spent by all map tasks (ms)=21529\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5822\n",
      "\t\tTotal vcore-seconds taken by all map tasks=21529\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5822\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22045696\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5961728\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=830\n",
      "\t\tMap output records=823\n",
      "\t\tMap output bytes=7678\n",
      "\t\tMap output materialized bytes=34\n",
      "\t\tInput split bytes=248\n",
      "\t\tCombine input records=823\n",
      "\t\tCombine output records=2\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=34\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=359\n",
      "\t\tCPU time spent (ms)=1800\n",
      "\t\tPhysical memory (bytes) snapshot=670498816\n",
      "\t\tVirtual memory (bytes) snapshot=4512378880\n",
      "\t\tTotal committed heap usage (bytes)=673390592\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=3\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=45835\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9\n",
      "16/08/31 02:42:30 INFO streaming.StreamJob: Output directory: lowest_temp-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r lowest_temp-output\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.4.2.jar \\\n",
    "   -mapper /home/cloudera/DAMLASB/map_reduce/lowest_temp/mapper.py \\\n",
    "   -reducer /home/cloudera/DAMLASB/map_reduce/lowest_temp/reducer.py \\\n",
    "   -combiner /home/cloudera/DAMLASB/map_reduce/lowest_temp/reducer.py \\\n",
    "   -input uk_heathrow_input_file.txt \\\n",
    "   -output lowest_temp-output  \\\n",
    "   -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963\t0.0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat lowest_temp-output/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\tsir 2\n",
      "['hello', 'sir', '2']\n"
     ]
    }
   ],
   "source": [
    "x =\"hello\"\n",
    "y = \"sir\"\n",
    "z = \"2\"\n",
    "a = \"%s\\t%s %s\" % (x,y,z)\n",
    "print(a)\n",
    "print(a.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "man hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
